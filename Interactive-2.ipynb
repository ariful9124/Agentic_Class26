{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.11.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3f7e5",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Llama 3.2-1B MMLU Evaluation Script (Laptop Optimized with Quantization)\n",
    "\n",
    "This script evaluates Llama 3.2-1B on the MMLU benchmark.\n",
    "Optimized for laptops with 4-bit or 8-bit quantization to reduce memory usage.\n",
    "\n",
    "Quantization options:\n",
    "- 4-bit: ~1.5 GB VRAM/RAM (default for laptop)\n",
    "- 8-bit: ~2.5 GB VRAM/RAM\n",
    "- No quantization: ~5 GB VRAM/RAM\n",
    "\n",
    "Usage:\n",
    "1. Install: pip install transformers torch datasets accelerate tqdm bitsandbytes\n",
    "2. Login: huggingface-cli login\n",
    "3. Run: python llama_mmlu_eval_quantized.py\n",
    "\n",
    "Set QUANTIZATION_BITS below to choose quantization level.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import platform\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63cfbc",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these settings\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"allenai/OLMo-2-0425-1B\",\n",
    "    \"Qwen/Qwen2.5-0.5B\"\n",
    "]\n",
    "# GPU settings\n",
    "# If True, will attempt to use the best available GPU (CUDA for NVIDIA, MPS for Apple Silicon)\n",
    "# If False, will always use CPU regardless of available hardware\n",
    "USE_GPU = False  # Set to False to force CPU-only execution\n",
    "\n",
    "MAX_NEW_TOKENS = 1\n",
    "\n",
    "# Verbose output settings\n",
    "# If True, will print each question, model's answer, and correctness\n",
    "PRINT_QUESTIONS = False  # Set to True to see detailed output for each question\n",
    "\n",
    "# Quantization settings\n",
    "# Options: 4, 8, or None (default is None for full precision)\n",
    "#\n",
    "# To enable quantization, change QUANTIZATION_BITS to one of the following:\n",
    "#   QUANTIZATION_BITS = 4   # 4-bit quantization: ~1.5 GB memory (most memory efficient)\n",
    "#   QUANTIZATION_BITS = 8   # 8-bit quantization: ~2.5 GB memory (balanced quality/memory)\n",
    "#   QUANTIZATION_BITS = None  # No quantization: ~5 GB memory (full precision, best quality)\n",
    "#\n",
    "# Notes:\n",
    "# - Quantization requires the 'bitsandbytes' package: pip install bitsandbytes\n",
    "# - Quantization only works with CUDA (NVIDIA GPUs), not with Apple Metal (MPS)\n",
    "# - If using Apple Silicon, quantization will be automatically disabled\n",
    "\n",
    "QUANTIZATION_BITS = 4  # Change to 4 or 8 to enable quantization\n",
    "\n",
    "# For quick testing, you can reduce this list\n",
    "MMLU_SUBJECTS = [\n",
    "    # \"abstract_algebra\", \"anatomy\", \n",
    "    \"astronomy\", \"business_ethics\",\n",
    "    # \"clinical_knowledge\", \"college_biology\", \"college_chemistry\",\n",
    "    # # \"college_computer_science\", \"college_mathematics\", \"college_medicine\",\n",
    "    #  \"college_physics\", \"computer_security\", \"conceptual_physics\",\n",
    "     \"econometrics\", \"electrical_engineering\", \"elementary_mathematics\",\n",
    "    \"formal_logic\", \"global_facts\", \"high_school_biology\",\n",
    "    \"high_school_chemistry\", \"high_school_computer_science\",\n",
    "    # # \"high_school_european_history\", \"high_school_geography\",\n",
    "    # # \"high_school_government_and_politics\", \"high_school_macroeconomics\",\n",
    "    #  \"high_school_mathematics\", \"high_school_microeconomics\",\n",
    "    #  \"high_school_physics\", \"high_school_psychology\", \"high_school_statistics\",\n",
    "    # \"high_school_us_history\", \"high_school_world_history\", \"human_aging\",\n",
    "    # \"human_sexuality\", \"international_law\", \"jurisprudence\",\n",
    "    # \"logical_fallacies\", \"machine_learning\", \"management\", \"marketing\",\n",
    "    # \"medical_genetics\", \"miscellaneous\", \"moral_disputes\", \"moral_scenarios\",\n",
    "    # \"nutrition\", \"philosophy\", \"prehistory\", \"professional_accounting\",\n",
    "    # \"professional_law\", \"professional_medicine\", \"professional_psychology\",\n",
    "    # \"public_relations\", \"security_studies\", \"sociology\", \"us_foreign_policy\",\n",
    "    # \"virology\", \"world_religions\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145b810",
   "metadata": {},
   "source": [
    "# UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321210cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_device():\n",
    "    \"\"\"Detect the best available device (CUDA, MPS, or CPU)\"\"\"\n",
    "\n",
    "    # If GPU is disabled, always use CPU\n",
    "    if not USE_GPU:\n",
    "        return \"cpu\"\n",
    "\n",
    "    # Check for CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "\n",
    "    # Check for Apple Silicon with Metal\n",
    "    if torch.backends.mps.is_available():\n",
    "        # Check if we're actually on Apple ARM\n",
    "        is_apple_arm = platform.system() == \"Darwin\" and platform.processor() == \"arm\"\n",
    "\n",
    "        if is_apple_arm:\n",
    "            # Metal is available but incompatible with quantization\n",
    "            if QUANTIZATION_BITS is not None:\n",
    "                print(\"\\n\" + \"=\"*70)\n",
    "                print(\"ERROR: Metal and Quantization Conflict\")\n",
    "                print(\"=\"*70)\n",
    "                print(\"Metal Performance Shaders (MPS) is incompatible with quantization.\")\n",
    "                print(f\"You have USE_GPU = True and QUANTIZATION_BITS = {QUANTIZATION_BITS}\")\n",
    "                print(\"\")\n",
    "                print(\"Please choose one of the following options:\")\n",
    "                print(\"  1. Set USE_GPU = False to use CPU with quantization\")\n",
    "                print(\"  2. Set QUANTIZATION_BITS = None to use Metal without quantization\")\n",
    "                print(\"=\"*70 + \"\\n\")\n",
    "                sys.exit(1)\n",
    "            return \"mps\"\n",
    "\n",
    "    # Default to CPU\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_environment():\n",
    "    global QUANTIZATION_BITS\n",
    "    \"\"\"Check environment and dependencies\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"Environment Check\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Check if in Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"√¢≈ì‚Äú Running in Google Colab\")\n",
    "        in_colab = True\n",
    "    except:\n",
    "        print(\"√¢≈ì‚Äú Running locally (not in Colab)\")\n",
    "        in_colab = False\n",
    "\n",
    "    # Check system info\n",
    "    print(f\"√¢≈ì‚Äú Platform: {platform.system()} ({platform.machine()})\")\n",
    "    if platform.system() == \"Darwin\":\n",
    "        print(f\"√¢≈ì‚Äú Processor: {platform.processor()}\")\n",
    "\n",
    "    # Detect and set device\n",
    "    device = detect_device()\n",
    "\n",
    "    # Check device\n",
    "    if device == \"cuda\":\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"√¢≈ì‚Äú GPU Available: {gpu_name}\")\n",
    "        print(f\"√¢≈ì‚Äú GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    elif device == \"mps\":\n",
    "        print(\"√¢≈ì‚Äú Apple Metal (MPS) Available\")\n",
    "        print(\"√¢≈ì‚Äú Using Metal Performance Shaders for GPU acceleration\")\n",
    "    else:\n",
    "        print(\"√¢≈° √Ø¬∏¬è  No GPU detected - running on CPU\")\n",
    "       \n",
    "    # Check quantization support\n",
    "\n",
    "    if QUANTIZATION_BITS is not None:\n",
    "        try:\n",
    "            import bitsandbytes\n",
    "            print(f\"√¢≈ì‚Äú bitsandbytes installed - {QUANTIZATION_BITS}-bit quantization available\")\n",
    "        except ImportError:\n",
    "            print(f\"√¢¬ù≈í bitsandbytes NOT installed - cannot use quantization\")\n",
    "            sys.exit(1)\n",
    "        if device == 'mps':\n",
    "            print(f\"√¢¬ù≈í Apple METAL is incompatible with quantization\")\n",
    "            print(\"√¢≈ì‚Äú Quantization disabled - loading full precision model\")\n",
    "            QUANTIZATION_BITS = None\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"√¢≈ì‚Äú Quantization disabled - loading full precision model\")\n",
    "    \n",
    "    # Check HF authentication\n",
    "    try:\n",
    "        from huggingface_hub import HfFolder\n",
    "        token = HfFolder.get_token()\n",
    "        if token:\n",
    "            print(\"√¢≈ì‚Äú Hugging Face authenticated\")\n",
    "        else:\n",
    "            print(\"√¢≈° √Ø¬∏¬è  No Hugging Face token found\")\n",
    "            print(\"Run: huggingface-cli login\")\n",
    "    except:\n",
    "        print(\"√¢≈° √Ø¬∏¬è  Could not check Hugging Face authentication\")\n",
    "    \n",
    "    # Print configuration\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Configuration\")\n",
    "    print(\"=\"*70)\n",
    "    # Note: model_name will be passed separately, not printed here\n",
    "    print(f\"Device: {device}\")\n",
    "    if QUANTIZATION_BITS is not None:\n",
    "        print(f\"Quantization: {QUANTIZATION_BITS}-bit\")\n",
    "        if QUANTIZATION_BITS == 4:\n",
    "            print(f\"Expected memory: ~1.5 GB\")\n",
    "        elif QUANTIZATION_BITS == 8:\n",
    "            print(f\"Expected memory: ~2.5 GB\")\n",
    "    else:\n",
    "        print(f\"Quantization: None (full precision)\")\n",
    "        if device == \"cuda\":\n",
    "            print(f\"Expected memory: ~5 GB (FP32)\")\n",
    "        elif device == \"mps\":\n",
    "            print(f\"Expected memory: ~2.5 GB (FP16)\")\n",
    "        else:\n",
    "            print(f\"Expected memory: ~5 GB (FP32)\")\n",
    "    print(f\"Number of subjects: {len(MMLU_SUBJECTS)}\")\n",
    "\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    return in_colab, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bf63a-75df-4e84-9397-0ae8156f9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantization_config():\n",
    "    \"\"\"Create quantization config based on settings\"\"\"\n",
    "    if QUANTIZATION_BITS is None:\n",
    "        return None\n",
    "    \n",
    "    if QUANTIZATION_BITS == 4:\n",
    "        # 4-bit quantization (most memory efficient)\n",
    "        config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,  # Double quantization for extra compression\n",
    "            bnb_4bit_quant_type=\"nf4\"  # NormalFloat4 - better for LLMs\n",
    "        )\n",
    "        print(\"Using 4-bit quantization (NF4 + double quant)\")\n",
    "        print(\"Memory usage: ~1.5 GB\")\n",
    "    elif QUANTIZATION_BITS == 8:\n",
    "        # 8-bit quantization (balanced)\n",
    "        config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False\n",
    "        )\n",
    "        print(\"Using 8-bit quantization\")\n",
    "        print(\"Memory usage: ~2.5 GB\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid QUANTIZATION_BITS: {QUANTIZATION_BITS}. Use 4, 8, or None\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_name, device):\n",
    "    \"\"\"Load model with optional quantization\"\"\"\n",
    "    print(f\"\\nLoading model {model_name}...\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(\"√¢≈ì‚Äú Tokenizer loaded\")\n",
    "\n",
    "        # Get quantization config\n",
    "        quant_config = get_quantization_config()\n",
    "\n",
    "        # Load model\n",
    "        print(\"Loading model (this may take 2-3 minutes)...\")\n",
    "\n",
    "        if quant_config is not None:\n",
    "            # Quantized model loading (only works with CUDA)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quant_config,\n",
    "                device_map=\"auto\",\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        else:\n",
    "            # Non-quantized model loading\n",
    "            if device == \"cuda\":\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    dtype=torch.float32,\n",
    "                    device_map=\"auto\",\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "            elif device == \"mps\":\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    dtype=torch.float16,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                model = model.to(device)\n",
    "            else:  # CPU\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    dtype=torch.float32,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                model = model.to(device)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Print model info\n",
    "        print(\"√¢≈ì‚Äú Model loaded successfully!\")\n",
    "        print(f\"  Model device: {next(model.parameters()).device}\")\n",
    "        print(f\"  Model dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "        # Check memory usage\n",
    "        if torch.cuda.is_available():\n",
    "            memory_allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "            memory_reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "            print(f\"  GPU Memory: {memory_allocated:.2f} GB allocated, {memory_reserved:.2f} GB reserved\")\n",
    "\n",
    "            # Check if using quantization\n",
    "            if quant_config is not None:\n",
    "                print(f\"  Quantization: {QUANTIZATION_BITS}-bit active\")\n",
    "        elif device == \"mps\":\n",
    "            print(f\"  Running on Apple Metal (MPS)\")\n",
    "\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n√¢¬ù≈í Error loading model: {e}\")\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"1. No Hugging Face token - Run: huggingface-cli login\")\n",
    "        print(\"2. Llama license not accepted - Visit: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\")\n",
    "        print(\"3. bitsandbytes not installed - Run: pip install bitsandbytes\")\n",
    "        print(\"4. Out of memory - Try 4-bit quantization or smaller model\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def format_mmlu_prompt(question, choices):\n",
    "    \"\"\"Format MMLU question as multiple choice\"\"\"\n",
    "    choice_labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    prompt = f\"{question}\\n\\n\"\n",
    "    for label, choice in zip(choice_labels, choices):\n",
    "        prompt += f\"{label}. {choice}\\n\"\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_model_prediction(model, tokenizer, prompt):\n",
    "    \"\"\"Get model's prediction for multiple-choice question\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            temperature=1.0\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    answer = generated_text.strip()[:1].upper()\n",
    "    \n",
    "    if answer not in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        for char in generated_text.upper():\n",
    "            if char in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                answer = char\n",
    "                break\n",
    "        else:\n",
    "            answer = \"A\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "def evaluate_subject(model, tokenizer, subject, print_examples=False):\n",
    "    \"\"\"Evaluate model on a specific MMLU subject\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        tokenizer: The associated tokenizer\n",
    "        subject: The MMLU subject string\n",
    "        print_examples (bool): If True, print question, model's answer, and correctness for each sample\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating subject: {subject}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        dataset = load_dataset(\"cais/mmlu\", subject, split=\"test\")\n",
    "    except Exception as e:\n",
    "        print(f\"√¢¬ù≈í Error loading subject {subject}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    question_details = []  # Store detailed question-level data\n",
    "    \n",
    "    for example in tqdm(dataset, desc=f\"Testing {subject}\", leave=True):\n",
    "        question = example[\"question\"]\n",
    "        choices = example[\"choices\"]\n",
    "        correct_answer_idx = example[\"answer\"]\n",
    "        correct_answer = [\"A\", \"B\", \"C\", \"D\"][correct_answer_idx]\n",
    "        \n",
    "        prompt = format_mmlu_prompt(question, choices)\n",
    "        predicted_answer = get_model_prediction(model, tokenizer, prompt)\n",
    "        \n",
    "        is_correct = predicted_answer == correct_answer\n",
    "        \n",
    "        # Store question details\n",
    "        question_details.append({\n",
    "            \"question\": question,\n",
    "            \"choices\": choices,\n",
    "            \"choice_a\": choices[0] if len(choices) > 0 else \"\",\n",
    "            \"choice_b\": choices[1] if len(choices) > 1 else \"\",\n",
    "            \"choice_c\": choices[2] if len(choices) > 2 else \"\",\n",
    "            \"choice_d\": choices[3] if len(choices) > 3 else \"\",\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "        \n",
    "        if print_examples:\n",
    "            print(\"=\"*40)\n",
    "            print(\"Q:\", question)\n",
    "            for label, choice in zip([\"A\", \"B\", \"C\", \"D\"], choices):\n",
    "                print(f\"  {label}. {choice}\")\n",
    "            print(f\"‚Üí Model answer: {predicted_answer} (Correct: {correct_answer})\")\n",
    "            print(\"Result:\", \"RIGHT ‚úì\" if is_correct else \"WRONG ‚úó\")\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = (correct / total * 100) if total > 0 else 0\n",
    "    print(f\"√¢≈ì‚Äú Result: {correct}/{total} correct = {accuracy:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"subject\": subject,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"question_details\": question_details  # Include detailed question data\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to agentic_env (Python 3.10.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a628e-cfb9-424d-b012-65544115dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_name):\n",
    "    \"\"\"Main evaluation function for a single model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"{model_name} MMLU Evaluation:\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # Check environment\n",
    "    in_colab, device = check_environment()\n",
    "\n",
    "    # Load model\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = []\n",
    "    total_correct = 0\n",
    "    total_questions = 0\n",
    "    all_question_data = []  # Collect all question-level data for dataframe\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Starting evaluation on {len(MMLU_SUBJECTS)} subjects\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Initialize timing\n",
    "    start_real_time = time.perf_counter()\n",
    "    start_cpu_time = time.process_time()\n",
    "    \n",
    "    # Initialize GPU timing if CUDA is available\n",
    "    gpu_time_available = False\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        gpu_start_event = torch.cuda.Event(enable_timing=True)\n",
    "        gpu_end_event = torch.cuda.Event(enable_timing=True)\n",
    "        gpu_start_event.record()\n",
    "        gpu_time_available = True\n",
    "    \n",
    "    for i, subject in enumerate(MMLU_SUBJECTS, 1):\n",
    "        print(f\"\\nProgress: {i}/{len(MMLU_SUBJECTS)} subjects\")\n",
    "        result = evaluate_subject(model, tokenizer, subject, print_examples=PRINT_QUESTIONS)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            total_correct += result[\"correct\"]\n",
    "            total_questions += result[\"total\"]\n",
    "            \n",
    "            # Collect question details with model and subject info\n",
    "            if \"question_details\" in result:\n",
    "                for q_detail in result[\"question_details\"]:\n",
    "                    all_question_data.append({\n",
    "                        \"model\": model_name,\n",
    "                        \"subject\": subject,\n",
    "                        \"question\": q_detail[\"question\"],\n",
    "                        \"choice_a\": q_detail[\"choice_a\"],\n",
    "                        \"choice_b\": q_detail[\"choice_b\"],\n",
    "                        \"choice_c\": q_detail[\"choice_c\"],\n",
    "                        \"choice_d\": q_detail[\"choice_d\"],\n",
    "                        \"predicted_answer\": q_detail[\"predicted_answer\"],\n",
    "                        \"correct_answer\": q_detail[\"correct_answer\"],\n",
    "                        \"is_correct\": q_detail[\"is_correct\"]\n",
    "                    })\n",
    "    \n",
    "    # Calculate timing\n",
    "    end_real_time = time.perf_counter()\n",
    "    end_cpu_time = time.process_time()\n",
    "    \n",
    "    real_time_seconds = end_real_time - start_real_time\n",
    "    cpu_time_seconds = end_cpu_time - start_cpu_time\n",
    "    \n",
    "    # Get GPU time if available\n",
    "    gpu_time_seconds = None\n",
    "    if gpu_time_available:\n",
    "        gpu_end_event.record()\n",
    "        torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "        gpu_time_seconds = gpu_start_event.elapsed_time(gpu_end_event) / 1000.0  # Convert ms to seconds\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = (total_correct / total_questions * 100) if total_questions > 0 else 0\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Quantization: {QUANTIZATION_BITS}-bit\" if QUANTIZATION_BITS else \"None (full precision)\")\n",
    "    print(f\"Total Subjects: {len(results)}\")\n",
    "    print(f\"Total Questions: {total_questions}\")\n",
    "    print(f\"Total Correct: {total_correct}\")\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"TIMING INFORMATION\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Real Time: {real_time_seconds:.2f} seconds ({real_time_seconds/60:.2f} minutes)\")\n",
    "    print(f\"CPU Time: {cpu_time_seconds:.2f} seconds ({cpu_time_seconds/60:.2f} minutes)\")\n",
    "    if gpu_time_seconds is not None:\n",
    "        print(f\"GPU Time: {gpu_time_seconds:.2f} seconds ({gpu_time_seconds/60:.2f} minutes)\")\n",
    "    else:\n",
    "        print(f\"GPU Time: N/A (not using CUDA)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    quant_suffix = f\"_{QUANTIZATION_BITS}bit\" if QUANTIZATION_BITS else \"_full\"\n",
    "    # Sanitize model name for filename\n",
    "    model_name_safe = model_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    output_file = f\"{model_name_safe}_mmlu_results{quant_suffix}_{timestamp}.json\"\n",
    "    \n",
    "    output_data = {\n",
    "        \"model\": model_name,\n",
    "        \"quantization_bits\": QUANTIZATION_BITS,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"device\": str(device),\n",
    "        \"timing\": {\n",
    "            \"real_time_seconds\": real_time_seconds,\n",
    "            \"cpu_time_seconds\": cpu_time_seconds,\n",
    "            \"gpu_time_seconds\": gpu_time_seconds\n",
    "        },\n",
    "        \"overall_accuracy\": overall_accuracy,\n",
    "        \"total_correct\": total_correct,\n",
    "        \"total_questions\": total_questions,\n",
    "        \"subject_results\": results\n",
    "    }\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n√¢≈ì‚Äú Results saved to: {output_file}\")\n",
    "    \n",
    "    # Print top/bottom subjects\n",
    "    if len(results) > 0:\n",
    "        sorted_results = sorted(results, key=lambda x: x[\"accuracy\"], reverse=True)\n",
    "        \n",
    "        print(\"\\n√∞≈∏‚Äú≈† Top 5 Subjects:\")\n",
    "        for i, result in enumerate(sorted_results[:5], 1):\n",
    "            print(f\"  {i}. {result['subject']}: {result['accuracy']:.2f}%\")\n",
    "        \n",
    "        print(\"\\n√∞≈∏‚Äú‚Ä∞ Bottom 5 Subjects:\")\n",
    "        for i, result in enumerate(sorted_results[-5:], 1):\n",
    "            print(f\"  {i}. {result['subject']}: {result['accuracy']:.2f}%\")\n",
    "    \n",
    "    # Colab-specific instructions\n",
    "    if in_colab:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"√∞≈∏‚Äô¬æ To download results in Colab:\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"from google.colab import files\")\n",
    "        print(f\"files.download('{output_file}')\")\n",
    "    \n",
    "    print(\"\\n√¢≈ì‚Ä¶ Evaluation complete!\")\n",
    "    \n",
    "    # Create and save dataframe with all question details\n",
    "    df = None\n",
    "    csv_file = None\n",
    "    if all_question_data:\n",
    "        df = pd.DataFrame(all_question_data)\n",
    "        csv_file = f\"{model_name_safe}_mmlu_questions{quant_suffix}_{timestamp}.csv\"\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"√¢≈ì\" Question details saved to: {csv_file}\")\n",
    "        print(f\"   Total questions in dataframe: {len(df)}\")\n",
    "    \n",
    "    return output_file, df, csv_file\n",
    "\n",
    "\n",
    "def evaluate_all_models():\n",
    "    \"\"\"Evaluate all models in MODEL_NAMES list\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MULTI-MODEL EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Evaluating {len(MODEL_NAMES)} models:\")\n",
    "    for i, model_name in enumerate(MODEL_NAMES, 1):\n",
    "        print(f\"  {i}. {model_name}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    all_output_files = []\n",
    "    all_results = []\n",
    "    all_dataframes = []  # Collect dataframes from all models\n",
    "    \n",
    "    for i, model_name in enumerate(MODEL_NAMES, 1):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"MODEL {i}/{len(MODEL_NAMES)}: {model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            output_file, df, csv_file = main(model_name)\n",
    "            all_output_files.append(output_file)\n",
    "            all_results.append({\n",
    "                \"model\": model_name,\n",
    "                \"output_file\": output_file,\n",
    "                \"csv_file\": csv_file,\n",
    "                \"status\": \"success\"\n",
    "            })\n",
    "            if df is not None and len(df) > 0:\n",
    "                all_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n√¢¬ù≈í Error evaluating {model_name}: {e}\")\n",
    "            all_results.append({\n",
    "                \"model\": model_name,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Combine all dataframes into one\n",
    "    combined_df = None\n",
    "    combined_csv_file = None\n",
    "    if all_dataframes:\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        combined_csv_file = f\"all_models_mmlu_questions_{timestamp}.csv\"\n",
    "        combined_df.to_csv(combined_csv_file, index=False)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMBINED DATAFRAME CREATED\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Combined CSV file: {combined_csv_file}\")\n",
    "        print(f\"Total questions across all models: {len(combined_df)}\")\n",
    "        print(f\"Models included: {combined_df['model'].unique().tolist()}\")\n",
    "        print(f\"Subjects included: {len(combined_df['subject'].unique())}\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY - ALL MODELS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total models evaluated: {len(MODEL_NAMES)}\")\n",
    "    successful = sum(1 for r in all_results if r[\"status\"] == \"success\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {len(all_results) - successful}\")\n",
    "    print(\"\\nResults files:\")\n",
    "    for result in all_results:\n",
    "        if result[\"status\"] == \"success\":\n",
    "            print(f\"  ‚úì {result['model']}: {result['output_file']}\")\n",
    "            if result.get('csv_file'):\n",
    "                print(f\"    CSV: {result['csv_file']}\")\n",
    "        else:\n",
    "            print(f\"  √¢¬ù≈í {result['model']}: FAILED - {result.get('error', 'Unknown error')}\")\n",
    "    if combined_csv_file:\n",
    "        print(f\"\\n  üìä Combined dataframe: {combined_csv_file}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return all_output_files, all_results, combined_df, combined_csv_file\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Evaluate all models in MODEL_NAMES\n",
    "        output_files, results, combined_df, combined_csv_file = evaluate_all_models()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n√¢≈° √Ø¬∏¬è  Evaluation interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n√¢¬ù≈í Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2bcddc",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187b08db",
   "metadata": {},
   "source": [
    "## Step 5: Modify the code to do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run on a selection of 10 subjects using 2 other small models in addtion to Llama 3.2-1B.  \n",
    "### Add timing information to the evaluation summary showing the cycles consumed by each model.  Include all of real time, CPU time, and GPU time.\n",
    "### Add an option to the program to make it print out each question, the answer the model gives, and whether the answer is right or wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9dcf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use the *actual* result files, taken from the context file (file_context_0)\n",
    "json_files = [\n",
    "    \"meta-llama_Llama-3.2-1B-Instruct_mmlu_results_full_20260115_132328.json\",\n",
    "    \"Qwen_Qwen2.5-0.5B_mmlu_results_full_20260115_132536.json\",\n",
    "    \"allenai_OLMo-2-0425-1B_mmlu_results_full_20260115_132439.json\"\n",
    "]\n",
    "\n",
    "all_json_data = []\n",
    "for fname in json_files:\n",
    "    with open(fname, \"r\", encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        all_json_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7634e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>quantization_bits</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>device</th>\n",
       "      <th>timing</th>\n",
       "      <th>overall_accuracy</th>\n",
       "      <th>total_correct</th>\n",
       "      <th>total_questions</th>\n",
       "      <th>subject_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
       "      <td>None</td>\n",
       "      <td>20260115_132328</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{'real_time_seconds': 61.03539550001733, 'cpu_...</td>\n",
       "      <td>40.856481</td>\n",
       "      <td>706</td>\n",
       "      <td>1728</td>\n",
       "      <td>[{'subject': 'astronomy', 'correct': 75, 'tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen/Qwen2.5-0.5B</td>\n",
       "      <td>None</td>\n",
       "      <td>20260115_132536</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{'real_time_seconds': 52.153785023023374, 'cpu...</td>\n",
       "      <td>39.988426</td>\n",
       "      <td>691</td>\n",
       "      <td>1728</td>\n",
       "      <td>[{'subject': 'astronomy', 'correct': 76, 'tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allenai/OLMo-2-0425-1B</td>\n",
       "      <td>None</td>\n",
       "      <td>20260115_132439</td>\n",
       "      <td>cuda</td>\n",
       "      <td>{'real_time_seconds': 60.01255718804896, 'cpu_...</td>\n",
       "      <td>32.638889</td>\n",
       "      <td>564</td>\n",
       "      <td>1728</td>\n",
       "      <td>[{'subject': 'astronomy', 'correct': 60, 'tota...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model quantization_bits        timestamp device  \\\n",
       "0  meta-llama/Llama-3.2-1B-Instruct              None  20260115_132328   cuda   \n",
       "1                 Qwen/Qwen2.5-0.5B              None  20260115_132536   cuda   \n",
       "2            allenai/OLMo-2-0425-1B              None  20260115_132439   cuda   \n",
       "\n",
       "                                              timing  overall_accuracy  \\\n",
       "0  {'real_time_seconds': 61.03539550001733, 'cpu_...         40.856481   \n",
       "1  {'real_time_seconds': 52.153785023023374, 'cpu...         39.988426   \n",
       "2  {'real_time_seconds': 60.01255718804896, 'cpu_...         32.638889   \n",
       "\n",
       "   total_correct  total_questions  \\\n",
       "0            706             1728   \n",
       "1            691             1728   \n",
       "2            564             1728   \n",
       "\n",
       "                                     subject_results  \n",
       "0  [{'subject': 'astronomy', 'correct': 75, 'tota...  \n",
       "1  [{'subject': 'astronomy', 'correct': 76, 'tota...  \n",
       "2  [{'subject': 'astronomy', 'correct': 60, 'tota...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame directly from the list of parsed JSON objects\n",
    "df = pd.DataFrame(all_json_data)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
