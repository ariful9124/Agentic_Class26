{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a25bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bare-Bones Chat Agent for Llama 3.2-1B-Instruct\n",
    "\n",
    "This is a minimal chat interface that demonstrates:\n",
    "1. How to load a model without quantization\n",
    "2. How chat history is maintained and fed back to the model\n",
    "3. The difference between plain text history and tokenized input\n",
    "\n",
    "No classes, no fancy features - just the essentials.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Change these settings as needed\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# System prompt - This sets the chatbot's behavior and personality\n",
    "# Change this to customize how the chatbot responds\n",
    "SYSTEM_PROMPT = \"You are a helpful AI assistant. Be concise and friendly.\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL (NO QUANTIZATION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading model (this takes 1-2 minutes)...\")\n",
    "\n",
    "# Load tokenizer (converts text to numbers and vice versa)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load model in half precision (float16) for efficiency\n",
    "# Use float16 on GPU, or float32 on CPU if needed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float16,                        # Use FP16 for efficiency\n",
    "    device_map=\"auto\",                          # Automatically choose GPU/CPU\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model.eval()  # Set to evaluation mode (no training)\n",
    "print(f\"âœ“ Model loaded! Using device: {model.device}\")\n",
    "print(f\"âœ“ Memory usage: ~2.5 GB (FP16)\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHAT HISTORY - This is stored as PLAIN TEXT (list of dictionaries)\n",
    "# ============================================================================\n",
    "# The chat history is a list of messages in this format:\n",
    "# [\n",
    "#   {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "#   {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "#   {\"role\": \"assistant\", \"content\": \"Hi! How can I help?\"},\n",
    "#   {\"role\": \"user\", \"content\": \"What's 2+2?\"},\n",
    "#   {\"role\": \"assistant\", \"content\": \"2+2 equals 4.\"}\n",
    "# ]\n",
    "#\n",
    "# This is PLAIN TEXT - humans can read it\n",
    "# The model CANNOT use this directly - it needs to be tokenized first\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# Add system prompt to history (this persists across the entire conversation)\n",
    "chat_history.append({\n",
    "    \"role\": \"system\",\n",
    "    \"content\": SYSTEM_PROMPT\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# CHAT LOOP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Chat started! Type 'quit' or 'exit' to end the conversation.\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "    # ========================================================================\n",
    "    # STEP 1: Get user input (PLAIN TEXT)\n",
    "    # ========================================================================\n",
    "    user_input = input(\"You: \").strip()\n",
    "    \n",
    "    # Check for exit commands\n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"\\nGoodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Skip empty inputs\n",
    "    if not user_input:\n",
    "        continue\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Add user message to chat history (PLAIN TEXT)\n",
    "    # ========================================================================\n",
    "    # The chat history grows with each exchange\n",
    "    # We append the new user message to the existing history\n",
    "    chat_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_input\n",
    "    })\n",
    "    \n",
    "    # At this point, chat_history looks like:\n",
    "    # [\n",
    "    #   {\"role\": \"system\", \"content\": \"You are helpful...\"},\n",
    "    #   {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    #   {\"role\": \"assistant\", \"content\": \"Hi!\"},\n",
    "    #   {\"role\": \"user\", \"content\": \"What's 2+2?\"},      â† Just added\n",
    "    # ]\n",
    "    # This is still PLAIN TEXT\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Convert chat history to model input (TOKENIZATION)\n",
    "    # ========================================================================\n",
    "    # The model needs numbers (tokens), not text\n",
    "    # apply_chat_template() does two things:\n",
    "    #   1. Formats the chat history with special tokens (like <|start|>, <|end|>)\n",
    "    #   2. Converts the formatted text into token IDs (numbers)\n",
    "    \n",
    "    # First, apply_chat_template formats the history and converts to tokens\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        chat_history,                    # Our PLAIN TEXT history\n",
    "        add_generation_prompt=True,      # Add prompt for assistant's response\n",
    "        return_tensors=\"pt\"              # Return as PyTorch tensor (numbers)\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Create attention mask (1 for all tokens since we have no padding)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    # Now input_ids is TOKENIZED - it's a tensor of numbers like:\n",
    "    # tensor([[128000, 128006, 9125, 128007, 271, 2675, 527, 264, ...]])\n",
    "    # These numbers represent our entire conversation history\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 4: Generate assistant response (MODEL INFERENCE)\n",
    "    # ========================================================================\n",
    "    # The model looks at the ENTIRE chat history (in tokenized form)\n",
    "    # and generates a response\n",
    "\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    with torch.no_grad():  # Don't calculate gradients (we're not training)\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,   # Explicitly pass attention mask\n",
    "            max_new_tokens=512,              # Maximum length of response\n",
    "            do_sample=True,                  # Use sampling for variety\n",
    "            temperature=0.7,                 # Lower = more focused, higher = more random\n",
    "            top_p=0.9,                       # Nucleus sampling\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # outputs contains: [original input tokens + new generated tokens]\n",
    "    # We only want the NEW tokens (the assistant's response)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 5: Decode the response (DETOKENIZATION)\n",
    "    # ========================================================================\n",
    "    # Extract only the newly generated tokens\n",
    "    new_tokens = outputs[0][input_ids.shape[1]:]\n",
    "    \n",
    "    # Convert tokens (numbers) back to text (PLAIN TEXT)\n",
    "    assistant_response = tokenizer.decode(\n",
    "        new_tokens,\n",
    "        skip_special_tokens=True  # Remove special tokens like <|end|>\n",
    "    )\n",
    "    \n",
    "    print(assistant_response)  # Display the response\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 6: Add assistant response to chat history (PLAIN TEXT)\n",
    "    # ========================================================================\n",
    "    # This is crucial! We add the assistant's response to the history\n",
    "    # so the model remembers what it said in future turns\n",
    "    \n",
    "    chat_history.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": assistant_response\n",
    "    })\n",
    "    \n",
    "    # Now chat_history has grown again:\n",
    "    # [\n",
    "    #   {\"role\": \"system\", \"content\": \"You are helpful...\"},\n",
    "    #   {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    #   {\"role\": \"assistant\", \"content\": \"Hi!\"},\n",
    "    #   {\"role\": \"user\", \"content\": \"What's 2+2?\"},\n",
    "    #   {\"role\": \"assistant\", \"content\": \"4\"}              â† Just added\n",
    "    # ]\n",
    "    \n",
    "    # When the loop repeats:\n",
    "    # - User enters new message\n",
    "    # - We add it to chat_history\n",
    "    # - We tokenize the ENTIRE history (including all previous exchanges)\n",
    "    # - Model sees everything and generates response\n",
    "    # - We add response to history\n",
    "    # - Repeat...\n",
    "    \n",
    "    # This is how the chatbot \"remembers\" the conversation!\n",
    "    # Each turn, we feed it the ENTIRE conversation history\n",
    "    \n",
    "    print()  # Blank line for readability\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
