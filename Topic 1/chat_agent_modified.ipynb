{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eb9544c",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb79177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The waved yellow underlines in Python editors (such as VSCode, Jupyter, PyCharm) typically indicate warnings from linters or static analyzers,\n",
    "# such as unused imports, unresolved references, or style issues.\n",
    "# While you can't remove them with code alone, you can address them by removing unused imports, correct errors, or configure your linter to ignore certain warnings.\n",
    "# Below is a cleaned version with only commonly-used imports retained. \n",
    "# Adjust as necessary based on the actual usage in your notebook!\n",
    "\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import pandas as pd\n",
    "from utils.contx_logger import ContextMetricsLogger\n",
    "from utils.contx_management import hierarchical_manage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c6bfa",
   "metadata": {},
   "source": [
    "# Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6592dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "HISTORY_ON = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdce77c",
   "metadata": {},
   "source": [
    "## System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7acce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt - This sets the chatbot's behavior and personality\n",
    "# Change this to customize how the chatbot responds\n",
    "SYSTEM_PROMPT = \"You are a helpful AI assistant. Be concise and friendly.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9a8e4",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa6eccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model (this takes 1-2 minutes)...\n",
      "âœ“ Model loaded! Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD MODEL (NO QUANTIZATION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading model (this takes 1-2 minutes)...\")\n",
    "\n",
    "# Load tokenizer (converts text to numbers and vice versa)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load model in half precision (float16) for efficiency\n",
    "# Use float16 on GPU, or float32 on CPU if needed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float16,                        # Use FP16 for efficiency\n",
    "    device_map=\"auto\",                          # Automatically choose GPU/CPU\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "model.eval()  # Set to evaluation mode (no training)\n",
    "print(f\"âœ“ Model loaded! Using device: {model.device}\")\n",
    "# print(f\"âœ“ Memory usage: ~2.5 GB (FP16)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50baec",
   "metadata": {},
   "source": [
    "# Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7400d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_logger = ContextMetricsLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# CHAT LOOP (with hierarchical/hybrid context management)\n",
    "# ========================================================================\n",
    "\n",
    "chat_history = []\n",
    "chat_history.append({\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Chat started! Type 'quit' or 'exit' to end the conversation.\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "\n",
    "    user_input = input(\"You: \").strip()\n",
    "\n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"\\nGoodbye!\")\n",
    "        break\n",
    "\n",
    "    if not user_input:\n",
    "        continue\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # NEW: Build the \"working_history\" to feed into the model\n",
    "    # - If HISTORY_ON: use hierarchical_manage (L0/L1/L2)\n",
    "    # - If HISTORY_OFF: only system + current user message\n",
    "    # --------------------------------------------------------------------\n",
    "    if HISTORY_ON:\n",
    "        system_msg = chat_history[0]  # always preserve system prompt\n",
    "        working_history, summary_l1, summary_l2, raw_buffer, BUDGET = hierarchical_manage(system_msg, user_input, model, tokenizer)\n",
    "    else:\n",
    "        working_history = [chat_history[0], {\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Tokenize working_history (NOT chat_history)\n",
    "    # --------------------------------------------------------------------\n",
    "    encoded = tokenizer.apply_chat_template(\n",
    "        working_history,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True\n",
    "    )\n",
    "    input_ids = encoded[\"input_ids\"].to(model.device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    new_tokens = outputs[0][input_ids.shape[1]:]\n",
    "    assistant_response = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    print(assistant_response)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Collect metrics\n",
    "    latency = time.time() - t0\n",
    "    input_tokens = input_ids.shape[1]\n",
    "    output_tokens = new_tokens.numel()\n",
    "    # Estimate L1/L2 token sizes\n",
    "    l1_tokens = len(tokenizer.encode(summary_l1)) if summary_l1 else 0\n",
    "    l2_tokens = len(tokenizer.encode(summary_l2)) if summary_l2 else 0\n",
    "    # Detect summarization trigger (simple heuristic)\n",
    "    summarization_triggered = (l1_tokens > 0 or l2_tokens > 0)\n",
    "\n",
    "    metrics_logger.log(\n",
    "        input_tokens=input_tokens,\n",
    "        output_tokens=output_tokens,\n",
    "        token_budget=BUDGET,\n",
    "        summarization_triggered=summarization_triggered,\n",
    "        l1_tokens=l1_tokens,\n",
    "        l2_tokens=l2_tokens,\n",
    "        raw_buffer_len=len(raw_buffer),\n",
    "        latency_seconds=latency\n",
    "    )\n",
    "    # --------------------------------------------------------------------\n",
    "    # NEW: Store conversation\n",
    "    # - chat_history can still store full log (optional)\n",
    "    # - raw_buffer is what hierarchical_manage uses as \"recent verbatim memory\"\n",
    "    # --------------------------------------------------------------------\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    if HISTORY_ON:\n",
    "        raw_buffer.append({\"role\": \"user\", \"content\": user_input})\n",
    "        raw_buffer.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        # Optional: keep raw_buffer from growing too large even before summarization triggers\n",
    "        if len(raw_buffer) > 40:  # keep last 40 messages (~20 turns)\n",
    "            raw_buffer = raw_buffer[-40:]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
