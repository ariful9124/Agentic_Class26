{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9df22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vl_langgraph_agent.py\n",
    "Exercise 1: Vision-Language LangGraph Chat Agent (Gradio + Ollama LLaVA)\n",
    "  \n",
    "What this does\n",
    "- Upload ONE image\n",
    "- Multi-turn chat about that image\n",
    "- LangGraph manages state (image + messages)\n",
    "- Gradio provides a polished web UI (works locally + notebooks; Colab with share=True)\n",
    "- Auto-downscales image if large (helps speed)\n",
    "\n",
    "Prereqs\n",
    "1) Install Ollama: https://ollama.com\n",
    "2) Pull a vision model, e.g.:\n",
    "     ollama pull llava\n",
    "\n",
    "Python deps\n",
    "  pip install gradio langgraph pillow ollama\n",
    "\n",
    "Run\n",
    "  python vl_langgraph_agent.py\n",
    "\n",
    "Notes\n",
    "- If slow: reduce MAX_IMAGE_SIDE (e.g., 512) and/or JPEG_QUALITY (e.g., 70).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from typing import TypedDict, List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "\n",
    "import ollama  # pip install ollama\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "OLLAMA_MODEL = \"llava\"     # change if needed\n",
    "MAX_IMAGE_SIDE = 768       # try 512 if slow\n",
    "JPEG_QUALITY = 85          # try 70 if slow / large uploads\n",
    "\n",
    "# âœ… NEW: Lines to append after every agent response\n",
    "POST_RESPONSE_LINES = \"\\n\\n---\\n\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def downscale_image(img: Image.Image, max_side: int = MAX_IMAGE_SIDE) -> Image.Image:\n",
    "    \"\"\"Downscale so max(width,height) <= max_side, preserving aspect ratio.\"\"\"\n",
    "    img = img.convert(\"RGB\")\n",
    "    w, h = img.size\n",
    "    m = max(w, h)\n",
    "    if m <= max_side:\n",
    "        return img\n",
    "    scale = max_side / float(m)\n",
    "    new_size = (max(1, int(w * scale)), max(1, int(h * scale)))\n",
    "    return img.resize(new_size, Image.BICUBIC)\n",
    "\n",
    "\n",
    "def image_to_b64_jpeg(img: Image.Image, quality: int = JPEG_QUALITY) -> str:\n",
    "    \"\"\"Encode PIL image as base64 JPEG (for Ollama vision models).\"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format=\"JPEG\", quality=quality, optimize=True)\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def msg_role_content(m: Any) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Accepts either:\n",
    "      - dict like {\"role\":\"user\",\"content\":\"...\"}\n",
    "      - LangChain message objects: SystemMessage / HumanMessage / AIMessage / ToolMessage\n",
    "    Returns (role, content) using OpenAI-ish roles: system|user|assistant|tool\n",
    "    \"\"\"\n",
    "    # dict case\n",
    "    if isinstance(m, dict):\n",
    "        role = m.get(\"role\", \"\") or \"\"\n",
    "        content = m.get(\"content\", \"\") or \"\"\n",
    "        return role, content\n",
    "\n",
    "    # object case\n",
    "    # sometimes .role exists\n",
    "    role = getattr(m, \"role\", None)\n",
    "    if role:\n",
    "        content = getattr(m, \"content\", \"\") or \"\"\n",
    "        return str(role), str(content)\n",
    "\n",
    "    # LangChain BaseMessage uses .type (system/human/ai/tool) and .content\n",
    "    mtype = getattr(m, \"type\", \"\") or \"\"\n",
    "    content = getattr(m, \"content\", \"\") or \"\"\n",
    "\n",
    "    type_map = {\"human\": \"user\", \"ai\": \"assistant\", \"system\": \"system\", \"tool\": \"tool\"}\n",
    "    role = type_map.get(str(mtype), str(mtype) if mtype else \"assistant\")\n",
    "    return role, str(content)\n",
    "\n",
    "\n",
    "def to_gradio_chat(messages: List[Any]):\n",
    "    \"\"\"\n",
    "    Return Gradio Chatbot history in \"messages format\" using ChatMessage objects.\n",
    "    This is the most compatible across Gradio versions when type=\"messages\".\n",
    "    \"\"\"\n",
    "    chat = []\n",
    "    for m in messages:\n",
    "        role, content = msg_role_content(m)\n",
    "\n",
    "        # Normalize role\n",
    "        if role == \"human\":\n",
    "            role = \"user\"\n",
    "        elif role == \"ai\":\n",
    "            role = \"assistant\"\n",
    "\n",
    "        # Only show user/assistant turns in the UI\n",
    "        if role not in (\"user\", \"assistant\"):\n",
    "            continue\n",
    "\n",
    "        chat.append(gr.ChatMessage(role=role, content=str(content)))\n",
    "    return chat\n",
    "\n",
    "\n",
    "def merge_state(prev: Dict[str, Any], delta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Merge LangGraph node output into persistent UI state.\n",
    "    Special case: messages are appended using add_messages reducer.\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = dict(prev)\n",
    "\n",
    "    if \"messages\" in delta:\n",
    "        out[\"messages\"] = add_messages(out.get(\"messages\", []), delta[\"messages\"])\n",
    "\n",
    "    for k, v in delta.items():\n",
    "        if k == \"messages\":\n",
    "            continue\n",
    "        out[k] = v\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LangGraph State\n",
    "# -----------------------------\n",
    "class AgentState(TypedDict, total=False):\n",
    "    # image payload\n",
    "    image_b64: str\n",
    "    has_image: bool\n",
    "\n",
    "    # conversation (mixed dict / LC message objects)\n",
    "    messages: List[Any]\n",
    "\n",
    "    # controls\n",
    "    user_text: str\n",
    "    should_reset: bool\n",
    "\n",
    "    # output\n",
    "    assistant_text: str\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful vision-language assistant. \"\n",
    "    \"Answer based ONLY on the uploaded image and the conversation context. \"\n",
    "    \"If something is not visible, say so.\"\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Graph Nodes\n",
    "# -----------------------------\n",
    "def node_set_image(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Ensure an image exists before chatting.\"\"\"\n",
    "    if not state.get(\"image_b64\"):\n",
    "        msg = \"Please upload an image first, then ask a question about it.\"\n",
    "        return {\n",
    "            \"has_image\": False,\n",
    "            \"assistant_text\": msg,\n",
    "            \"messages\": [{\"role\": \"assistant\", \"content\": msg}],\n",
    "        }\n",
    "    return {\"has_image\": True}\n",
    "\n",
    "\n",
    "def node_maybe_reset(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Reset chat history (keep image). Also seeds system message on first run.\"\"\"\n",
    "    if state.get(\"should_reset\", False):\n",
    "        return {\n",
    "            \"messages\": [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}],\n",
    "            \"assistant_text\": \"\",\n",
    "            \"user_text\": \"\",\n",
    "            \"should_reset\": False,\n",
    "        }\n",
    "\n",
    "    if not state.get(\"messages\"):\n",
    "        return {\"messages\": [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]}\n",
    "\n",
    "    return {}\n",
    "\n",
    "\n",
    "def node_add_user_message(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Append user message.\"\"\"\n",
    "    txt = (state.get(\"user_text\") or \"\").strip()\n",
    "    if not txt:\n",
    "        return {\"assistant_text\": \"Type a question to continue.\"}\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": txt}]}\n",
    "\n",
    "\n",
    "def node_call_vlm(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Call Ollama vision model with full conversation; attach image to latest user turn.\"\"\"\n",
    "    if not state.get(\"has_image\", False):\n",
    "        return {\"assistant_text\": state.get(\"assistant_text\", \"Please upload an image first.\")}\n",
    "\n",
    "    messages = state.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        return {\"assistant_text\": \"Internal error: missing messages.\"}\n",
    "\n",
    "    img_b64 = state[\"image_b64\"]\n",
    "\n",
    "    # Find last user message index\n",
    "    last_user_idx = None\n",
    "    for i in range(len(messages) - 1, -1, -1):\n",
    "        role, _ = msg_role_content(messages[i])\n",
    "        if role == \"user\":\n",
    "            last_user_idx = i\n",
    "            break\n",
    "\n",
    "    # Build Ollama messages as dicts\n",
    "    ollama_msgs: List[Dict[str, Any]] = []\n",
    "    for i, m in enumerate(messages):\n",
    "        role, content = msg_role_content(m)\n",
    "        om: Dict[str, Any] = {\"role\": role, \"content\": content}\n",
    "        if i == last_user_idx:\n",
    "            om[\"images\"] = [img_b64]\n",
    "        ollama_msgs.append(om)\n",
    "\n",
    "    try:\n",
    "        resp = ollama.chat(model=OLLAMA_MODEL, messages=ollama_msgs)\n",
    "        assistant_text = resp[\"message\"][\"content\"]\n",
    "\n",
    "        # âœ… NEW: append lines after response\n",
    "        assistant_text = assistant_text.rstrip() + POST_RESPONSE_LINES\n",
    "\n",
    "    except Exception as e:\n",
    "        assistant_text = f\"Error calling Ollama model '{OLLAMA_MODEL}': {e}\"\n",
    "\n",
    "    return {\n",
    "        \"assistant_text\": assistant_text,\n",
    "        \"messages\": [{\"role\": \"assistant\", \"content\": assistant_text}],\n",
    "        \"user_text\": \"\",\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_image_check(state: AgentState) -> str:\n",
    "    if not state.get(\"has_image\", False):\n",
    "        return END\n",
    "    return \"maybe_reset\"\n",
    "\n",
    "\n",
    "def route_after_maybe_reset(state: AgentState) -> str:\n",
    "    txt = (state.get(\"user_text\") or \"\").strip()\n",
    "    if not txt:\n",
    "        return END\n",
    "    return \"add_user\"\n",
    "\n",
    "\n",
    "def build_graph():\n",
    "    g = StateGraph(AgentState)\n",
    "    g.add_node(\"set_image\", node_set_image)\n",
    "    g.add_node(\"maybe_reset\", node_maybe_reset)\n",
    "    g.add_node(\"add_user\", node_add_user_message)\n",
    "    g.add_node(\"call_vlm\", node_call_vlm)\n",
    "\n",
    "    g.add_edge(START, \"set_image\")\n",
    "\n",
    "    g.add_conditional_edges(\n",
    "        \"set_image\",\n",
    "        route_after_image_check,\n",
    "        {\"maybe_reset\": \"maybe_reset\", END: END},\n",
    "    )\n",
    "\n",
    "    g.add_conditional_edges(\n",
    "        \"maybe_reset\",\n",
    "        route_after_maybe_reset,\n",
    "        {\"add_user\": \"add_user\", END: END},\n",
    "    )\n",
    "\n",
    "    g.add_edge(\"add_user\", \"call_vlm\")\n",
    "    g.add_edge(\"call_vlm\", END)\n",
    "\n",
    "    return g.compile()\n",
    "\n",
    "\n",
    "GRAPH = build_graph()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Gradio callbacks\n",
    "# -----------------------------\n",
    "def on_image_change(img: Image.Image, st: Optional[Dict[str, Any]]):\n",
    "    st = st or {}\n",
    "\n",
    "    if img is None:\n",
    "        st.pop(\"image_b64\", None)\n",
    "        st[\"has_image\"] = False\n",
    "        return st, to_gradio_chat(st.get(\"messages\", [])), \"Image cleared.\"\n",
    "\n",
    "    img_small = downscale_image(img, MAX_IMAGE_SIDE)\n",
    "    st[\"image_b64\"] = image_to_b64_jpeg(img_small, JPEG_QUALITY)\n",
    "    st[\"has_image\"] = True\n",
    "\n",
    "    # Seed system message if needed\n",
    "    if not st.get(\"messages\"):\n",
    "        st[\"should_reset\"] = True\n",
    "        delta = GRAPH.invoke(st)\n",
    "        st = merge_state(st, delta)\n",
    "\n",
    "    return st, to_gradio_chat(st.get(\"messages\", [])), \"Image loaded. Ask a question!\"\n",
    "\n",
    "\n",
    "def on_reset(st: Optional[Dict[str, Any]]):\n",
    "    st = st or {}\n",
    "    st[\"should_reset\"] = True\n",
    "    delta = GRAPH.invoke(st)\n",
    "    st = merge_state(st, delta)\n",
    "    return st, to_gradio_chat(st.get(\"messages\", [])), \"Chat reset.\"\n",
    "\n",
    "\n",
    "def on_submit(user_text: str, st: Optional[Dict[str, Any]]):\n",
    "    st = st or {}\n",
    "    st[\"user_text\"] = user_text\n",
    "    st[\"should_reset\"] = False\n",
    "\n",
    "    delta = GRAPH.invoke(st)\n",
    "    st = merge_state(st, delta)\n",
    "\n",
    "    if not st.get(\"has_image\", False):\n",
    "        status = \"Upload an image first.\"\n",
    "    elif st.get(\"assistant_text\"):\n",
    "        status = \"Answered.\"\n",
    "    else:\n",
    "        status = \"Ready.\"\n",
    "\n",
    "    return \"\", st, to_gradio_chat(st.get(\"messages\", [])), status\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Gradio UI\n",
    "# -----------------------------\n",
    "def build_ui():\n",
    "    with gr.Blocks(title=\"Vision-Language LangGraph Agent (Ollama)\") as demo:\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "# ðŸ–¼ï¸ Vision-Language LangGraph Chat Agent (Ollama + Gradio)\n",
    "\n",
    "1) Upload an image  \n",
    "2) Ask multi-turn questions about the image  \n",
    "3) State + context are managed by **LangGraph**\n",
    "\n",
    "**Speed tips:** lower `MAX_IMAGE_SIDE` (e.g., 512) and/or `JPEG_QUALITY` (e.g., 70).\n",
    "            \"\"\".strip()\n",
    "        )\n",
    "\n",
    "        st = gr.State({})  # persistent state (AgentState)\n",
    "\n",
    "        with gr.Row():\n",
    "            img_in = gr.Image(type=\"pil\", label=\"Upload image\")\n",
    "            chatbot = gr.Chatbot(label=\"Chat\", height=420)\n",
    "\n",
    "        status = gr.Textbox(label=\"Status\", value=\"Upload an image to begin.\", interactive=False)\n",
    "\n",
    "        with gr.Row():\n",
    "            txt = gr.Textbox(label=\"Your message\", placeholder=\"Ask something about the image...\", scale=5)\n",
    "            send = gr.Button(\"Send\", scale=1)\n",
    "            reset = gr.Button(\"Reset Chat\", scale=1)\n",
    "\n",
    "        img_in.change(fn=on_image_change, inputs=[img_in, st], outputs=[st, chatbot, status])\n",
    "        reset.click(fn=on_reset, inputs=[st], outputs=[st, chatbot, status])\n",
    "\n",
    "        send.click(fn=on_submit, inputs=[txt, st], outputs=[txt, st, chatbot, status])\n",
    "        txt.submit(fn=on_submit, inputs=[txt, st], outputs=[txt, st, chatbot, status])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = build_ui()\n",
    "    # Local: http://127.0.0.1:7860\n",
    "    # Colab: app.launch(share=True)\n",
    "    # Robust port selection: use requested port if possible, but fall back on auto if in use\n",
    "    preferred_port = 7860\n",
    "    server_name = \"127.0.0.1\"\n",
    "    share = False  # Change True if in Colab\n",
    "\n",
    "    def is_port_in_use(port: int, host: str = \"127.0.0.1\") -> bool:\n",
    "        import socket\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            return s.connect_ex((host, port)) == 0\n",
    "\n",
    "    try:\n",
    "        if not is_port_in_use(preferred_port, server_name):\n",
    "            app.launch(server_name=server_name, server_port=preferred_port, share=share)\n",
    "        else:\n",
    "            print(f\"Port {preferred_port} is in use. Launching on a random open port (set by Gradio).\")\n",
    "            app.launch(server_name=server_name, server_port=None, share=share)\n",
    "    except OSError as e:\n",
    "        print(f\"Gradio launch error: {e}\")\n",
    "        print(\"Trying to launch on a random port.\")\n",
    "        app.launch(server_name=server_name, server_port=None, share=share)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
