{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686ba93f",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6430ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4eebcd",
   "metadata": {},
   "source": [
    "## User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f49ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph_image(graph, filename=\"lg_graph.png\"):\n",
    "    \"\"\"\n",
    "    Generate a Mermaid diagram of the graph and save it as a PNG image.\n",
    "    Uses the graph's built-in Mermaid export functionality.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the Mermaid PNG representation of the graph\n",
    "        # This requires the 'grandalf' package for rendering\n",
    "        png_data = graph.get_graph(xray=True).draw_mermaid_png()\n",
    "\n",
    "        # Write the PNG data to file\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(png_data)\n",
    "\n",
    "        print(f\"Graph image saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save graph image: {e}\")\n",
    "        print(\"You may need to install additional dependencies: pip install grandalf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f73a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"\n",
    "    Detect and return the best available compute device.\n",
    "    Returns 'cuda' for NVIDIA GPUs, 'mps' for Apple Silicon, or 'cpu' as fallback.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA (NVIDIA GPU) for inference\")\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using MPS (Apple Silicon) for inference\")\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        print(\"Using CPU for inference\")\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1df252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STATE DEFINITION\n",
    "# =============================================================================\n",
    "# The state is a TypedDict that flows through all nodes in the graph.\n",
    "# Each node can read from and write to specific fields in the state.\n",
    "# LangGraph automatically merges the returned dict from each node into the state.\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State object that flows through the LangGraph nodes.\n",
    "\n",
    "    Fields:\n",
    "    - user_input: The text entered by the user (set by get_user_input node)\n",
    "    - should_exit: Boolean flag indicating if user wants to quit (set by get_user_input node)\n",
    "    - llm_response: The response generated by the LLM (set by call_llm node)\n",
    "\n",
    "    State Flow:\n",
    "    1. Initial state: all fields empty/default\n",
    "    2. After get_user_input: user_input and should_exit are populated\n",
    "    3. After call_llm: llm_response is populated\n",
    "    4. After print_response: state unchanged (node only reads, doesn't write)\n",
    "\n",
    "    The graph loops continuously:\n",
    "        get_user_input -> [conditional] -> call_llm -> print_response -> get_user_input\n",
    "                              |\n",
    "                              +-> END (if user wants to quit)\n",
    "    \"\"\"\n",
    "    user_input: str\n",
    "    should_exit: bool\n",
    "    verbose: bool\n",
    "    is_empty: bool\n",
    "\n",
    "    use_qwen: bool\n",
    "    llm_response: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba159ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm():\n",
    "    \"\"\"\n",
    "    Create and configure the LLM using HuggingFace's transformers library.\n",
    "    Downloads llama-3.2-1B-Instruct from HuggingFace Hub and wraps it\n",
    "    for use with LangChain via HuggingFacePipeline.\n",
    "    \"\"\"\n",
    "    # Get the optimal device for inference\n",
    "    device = get_device()\n",
    "\n",
    "    # Model identifier on HuggingFace Hub\n",
    "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "    print(\"This may take a moment on first run as the model is downloaded...\")\n",
    "\n",
    "    # Load the tokenizer - converts text to tokens the model understands\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Load the model itself with appropriate settings for the device\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        dtype=torch.float16 if device != \"cpu\" else torch.float32,\n",
    "        device_map=device if device == \"cuda\" else None,\n",
    "    )\n",
    "\n",
    "    # Move model to MPS device if using Apple Silicon\n",
    "    if device == \"mps\":\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Create a text generation pipeline that combines model and tokenizer\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=32,  # Maximum tokens to generate in response\n",
    "        do_sample=True,      # Enable sampling for varied responses\n",
    "        temperature=0.7,     # Controls randomness (lower = more deterministic)\n",
    "        top_p=0.95,          # Nucleus sampling parameter\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Suppress pad_token_id warning\n",
    "    )\n",
    "\n",
    "    # Wrap the HuggingFace pipeline for use with LangChain\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac88e3",
   "metadata": {},
   "source": [
    "## User Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb2db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# NODE 1: get_user_input\n",
    "# =========================\n",
    "def get_user_input(state: AgentState) -> dict:\n",
    "\n",
    "    if state.get(\"verbose\", False):\n",
    "        print(\"[TRACE] Entering get_user_input\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Enter your text (or 'quit' to exit):\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\n> \", end=\"\")\n",
    "    user_input = input().strip()\n",
    "\n",
    "    # Exit command\n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"Goodbye!\")\n",
    "        return {\n",
    "            \"user_input\": user_input,\n",
    "            \"should_exit\": True\n",
    "        }\n",
    "\n",
    "    # Empty input -> mark as empty and do NOT call LLM\n",
    "    if user_input == \"\":\n",
    "        return {\n",
    "            \"user_input\": \"\",\n",
    "            \"should_exit\": False,\n",
    "            \"is_empty\": True\n",
    "        }\n",
    "    # Turn verbose ON\n",
    "    if user_input.lower() == \"verbose\":\n",
    "        print(\"Verbose mode ENABLED\")\n",
    "        return {\n",
    "            \"user_input\": \"\",\n",
    "            \"should_exit\": False,\n",
    "            \"verbose\": True\n",
    "        }\n",
    "\n",
    "    # Turn verbose OFF\n",
    "    if user_input.lower() == \"quiet\":\n",
    "        print(\"Verbose mode DISABLED\")\n",
    "        return {\n",
    "            \"user_input\": \"\",\n",
    "            \"should_exit\": False,\n",
    "            \"verbose\": False\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"user_input\": user_input,\n",
    "        \"should_exit\": False,\n",
    "        \"is_empty\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1bc8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langgraph_simple_agent.py\n",
    "# Program demonstrates use of LangGraph for a very simple agent.\n",
    "# It writes to stdout and asks the user to enter a line of text through stdin.\n",
    "# It passes the line to the LLM llama-3.2-1B-Instruct, then prints the\n",
    "# what the LLM returns as text to stdout.\n",
    "# The LLM should use Cuda if available, if not then if mps is available then use that,\n",
    "# otherwise use cpu.\n",
    "# After the LangGraph graph is created but before it executes, the program\n",
    "# uses the Mermaid library to write a image of the graph to the file lg_graph.png\n",
    "# The program gets the LLM llama-3.2-1B-Instruct from Hugging Face and wraps\n",
    "# it for LangChain using HuggingFacePipeline.\n",
    "# The code is commented in detail so a reader can understand each step.\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "\n",
    "# Determine the best available device for inference\n",
    "# Priority: CUDA (NVIDIA GPU) > MPS (Apple Silicon) > CPU\n",
    "\n",
    "def create_graph(llm):\n",
    "\n",
    "    # =========================\n",
    "    # NODE 2: call_llm\n",
    "    # =========================\n",
    "    def call_llm(state: AgentState) -> dict:\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering call_llm\")\n",
    "            print(f\"[TRACE] user_input = {state['user_input']}\")\n",
    "\n",
    "        user_input = state[\"user_input\"]\n",
    "        prompt = f\"User: {user_input}\\nAssistant:\"\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Prompt sent to LLM:\")\n",
    "            print(prompt)\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] LLM returned response\")\n",
    "\n",
    "        return {\"llm_response\": response}\n",
    "\n",
    "    # =========================\n",
    "    # NODE 3: print_response\n",
    "    # =========================\n",
    "    def print_response(state: AgentState) -> dict:\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Entering print_response\")\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"LLM Response:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(state[\"llm_response\"])\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Returning to get_user_input\")\n",
    "\n",
    "        return {}\n",
    "\n",
    "    # =========================\n",
    "    # ROUTING FUNCTION\n",
    "    # =========================\n",
    "    def route_after_input(state: AgentState) -> str:\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Routing after input\")\n",
    "\n",
    "        if state.get(\"should_exit\", False):\n",
    "            if state.get(\"verbose\", False):\n",
    "                print(\"[TRACE] Routing to END\")\n",
    "            return END\n",
    "        # 2) Empty -> self-loop back to get_user_input\n",
    "        if state.get(\"is_empty\", False):\n",
    "            if state.get(\"verbose\", False):\n",
    "                 print(\"[TRACE] Empty input detected. Routing back to get_user_input.\")\n",
    "            return \"get_user_input\"\n",
    "\n",
    "        if state.get(\"verbose\", False):\n",
    "            print(\"[TRACE] Routing to call_llm\")\n",
    "\n",
    "        return \"call_llm\"\n",
    "\n",
    "    # =========================\n",
    "    # GRAPH BUILDING\n",
    "    # =========================\n",
    "    graph_builder = StateGraph(AgentState)\n",
    "\n",
    "    graph_builder.add_node(\"get_user_input\", get_user_input)\n",
    "    graph_builder.add_node(\"call_llm\", call_llm)\n",
    "    graph_builder.add_node(\"print_response\", print_response)\n",
    "\n",
    "    graph_builder.add_edge(START, \"get_user_input\")\n",
    "\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"get_user_input\",\n",
    "        route_after_input,\n",
    "        {\n",
    "            \"call_llm\": \"call_llm\",\n",
    "            \"get_user_input\": \"get_user_input\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "\n",
    "    graph_builder.add_edge(\"call_llm\", \"print_response\")\n",
    "    graph_builder.add_edge(\"print_response\", \"get_user_input\")\n",
    "\n",
    "    graph = graph_builder.compile()\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8073c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that orchestrates the simple agent workflow:\n",
    "    1. Initialize the LLM\n",
    "    2. Create the LangGraph\n",
    "    3. Save the graph visualization\n",
    "    4. Run the graph once (it loops internally until user quits)\n",
    "\n",
    "    The graph handles all looping internally through its edge structure:\n",
    "    - get_user_input: Prompts and reads from stdin\n",
    "    - call_llm: Processes input through the LLM\n",
    "    - print_response: Outputs the response, then loops back to get_user_input\n",
    "\n",
    "    The graph only terminates when the user types 'quit', 'exit', or 'q'.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"LangGraph Simple Agent with Llama-3.2-1B-Instruct\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "    # Step 1: Create and configure the LLM\n",
    "    llm = create_llm()\n",
    "\n",
    "    # Step 2: Build the LangGraph with the LLM\n",
    "    print(\"\\nCreating LangGraph...\")\n",
    "    graph = create_graph(llm)\n",
    "    print(\"Graph created successfully!\")\n",
    "\n",
    "    # Step 3: Save a visual representation of the graph before execution\n",
    "    # This happens BEFORE any graph execution, showing the graph structure\n",
    "    print(\"\\nSaving graph visualization...\")\n",
    "    save_graph_image(graph)\n",
    "\n",
    "    # Step 4: Run the graph - it will loop internally until user quits\n",
    "    # Create initial state with empty/default values\n",
    "    # The graph will loop continuously, updating state as it goes:\n",
    "    #   - get_user_input displays banner, populates user_input and should_exit\n",
    "    #   - call_llm populates llm_response\n",
    "    #   - print_response displays output, then loops back to get_user_input\n",
    "    initial_state: AgentState = {\n",
    "        \"user_input\": \"\",\n",
    "        \"should_exit\": False,\n",
    "        \"llm_response\": \"\",\n",
    "        \"is_empty\": False,\n",
    "        \"verbose\": False\n",
    "    }\n",
    "\n",
    "    # Single invocation - the graph loops internally via print_response -> get_user_input\n",
    "    # The graph only exits when route_after_input returns END (user typed quit/exit/q)\n",
    "    graph.invoke(initial_state)\n",
    "\n",
    "# Entry point - only run main() if this script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
